# Rapport G√©n√©ral

## Table des mati√®res

- [D√©finitions](#d√©finitions)
  - [Thread](#thread)
  - [S√©maphore](#s√©maphore)
  - [Ressource critique](#ressource-critique)
  - [Section critique](#section-critique)
  - [Programmation parall√®le](#programmation-parall√®le)
  - [Programmation partag√©e](#programmation-partag√©e)
  - [Parall√©lisme de t√¢ches](#parall√©lisme-de-t√¢ches)
- [TP1 : Dossier de conception du TP Thread](#tp1--dossier-de-conception-du-tp-thread)
  - [Exercice 1](#exercice-1)
  - [Exercice 2](#exercice-2)
- [TP2 : Affichage synchronis√©](#tp2--affichage-synchronis√©)
  - [S√©maphore](#s√©maphore-1)
    - [Outils de Synchronisation](#outils-de-synchronisation)
    - [Impl√©mentation](#impl√©mentation)
    - [Exemple : Analogie avec un Phare](#exemple--analogie-avec-un-phare)
    - [Proc√©d√©](#proc√©d√©)
    - [Conclusion](#conclusion)
- [TP3 : Bo√Æte aux lettres](#tp3--bo√Æte-aux-lettres)
  - [Analyse et R√©solution](#analyse-et-r√©solution)
  - [Ce que j'ai appris](#ce-que-jai-appris)
  - [Conclusion](#conclusion)
- [TP4 (part1): Concepts avanc√©s et Monte Carlo](#tp4--concepts-avanc√©s-et-monte-carlo)
  - [Master / Worker](#master--worker)
  - [Future](#future)
  - [Acc√©l√©ration (Speedup)](#acc√©l√©ration-speedup)
  - [Scalabilit√©](#scalabilit√©)
  - [M√©thode de Monte Carlo](#m√©thode-de-monte-carlo)
  - [Work Stealing Pool](#work-stealing-pool)
  - [Application de l'API Concurrent](#application-de-lapi-concurrent)
  - [Analyse des performances de Monte Carlo](#analyse-des-performances-de-monte-carlo)
- [TP4 (part2): Programmation distribu√©e](#programmation-distribu√©e)
  - [Master / Worker en distribu√©](#master--worker-en-distribu√©)
  - [Analyse des Sockets JAVA](#analyse-des-sockets-java)
    - [Analyse MasterSocket.java](#analyse-mastersocketjava)
    - [Analyse WorkerSocket.java](#analyse-workersocketjava)
  - [Monte Carlo Master/Worker Socket](#monte-carlo-masterworker-socket)
  - [√âtude de la scalabilit√©](#√©tude-de-la-scalabilit√©)

---

## D√©finitions

### Thread
Un **thread** est la plus petite unit√© d'ex√©cution dans un programme. Il repr√©sente un flux d'instructions qui peut s'ex√©cuter ind√©pendamment tout en partageant des ressources comme la m√©moire et les fichiers avec d'autres threads du m√™me processus.

### S√©maphore
Un **s√©maphore** est une variable utilis√©e pour g√©rer l'acc√®s concurrent √† une ressource partag√©e. Il peut √™tre binaire (1 ou 0) ou comptant (avec une valeur enti√®re), et ses op√©rations principales sont :
- **`wait`** : R√©duit la valeur du s√©maphore et bloque si elle devient n√©gative.
- **`signal`** : Augmente la valeur et r√©veille un thread bloqu√© si n√©cessaire.

### Ressource critique
Une **ressource critique** est un √©l√©ment (m√©moire, fichier, p√©riph√©rique) partag√© entre plusieurs threads ou processus, dont l'acc√®s concurrent peut entra√Æner des incoh√©rences si non contr√¥l√©.

### Section critique
Une **section critique** est une portion du code o√π une ressource critique est acc√©d√©e. Il est essentiel d'assurer qu'une seule entit√© (thread ou processus) l'ex√©cute √† la fois pour √©viter des erreurs ou des incoh√©rences.

### Programmation parall√®le
La **programmation parall√®le** consiste √† ex√©cuter plusieurs t√¢ches en simultan√© sur des processeurs multiples, permettant d'acc√©l√©rer l'ex√©cution d'un programme en divisant les calculs ou t√¢ches.

### Programmation partag√©e
La **programmation partag√©e** implique l'utilisation d'une m√©moire unique accessible par plusieurs threads ou processus. Elle n√©cessite des m√©canismes de synchronisation pour √©viter les conflits.

### Parall√©lisme de t√¢ches
Le **parall√©lisme de t√¢ches** correspond √† l'ex√©cution simultan√©e de t√¢ches diff√©rentes. Contrairement au parall√©lisme de donn√©es, chaque t√¢che effectue une op√©ration distincte et ind√©pendante.

---

## TP1 : Dossier de conception du TP Thread

### Exercice 1

**Impl√©mentation :**
La classe `UnMobile` repr√©sente un mobile qui se d√©place dans une interface graphique en utilisant un thread. Sa m√©thode `run()` g√®re le mouvement par incr√©ments successifs, suivis de redessins avec `repaint()`.

**Ce que j'ai compris :**
Le fonctionnement d'un thread inclut :
- La gestion des boucles d'ex√©cution (dans `run`).
- L'utilisation de `sleep` pour r√©guler la vitesse.
- L'importance de redessiner r√©guli√®rement les objets graphiques pour simuler le mouvement.

### Exercice 2

**Impl√©mentation :**
Un mobile est ajout√© √† une fen√™tre avec `add()` dans la classe `UneFenetre`, et un thread est lanc√© pour animer le mobile.

**Ce que j'ai appris :**
La manipulation de fen√™tres avec `JFrame`, notamment :
- La gestion de la visibilit√© (`setVisible`).
- L'ajout de composants dynamiques (comme `UnMobile`).
- L'utilisation de threads pour animer les composants.

---

## TP2 : Affichage synchronis√©

### S√©maphore

#### Outils de Synchronisation
Les s√©maphores sont utilis√©s pour g√©rer l'acc√®s s√©curis√© aux ressources partag√©es. Ils emp√™chent plusieurs threads d'acc√©der simultan√©ment √† une **section critique**, ce qui pourrait entra√Æner des conditions de course.

#### Impl√©mentation

1. **Initialisation** : Choisir la valeur de d√©part en fonction des besoins (1 pour exclusivit√©, >1 pour ressources multiples).
2. **Interblocages** : Pr√©venir les deadlocks en respectant l'ordre des verrous.
3. **Performance** : Minimiser les sections critiques pour limiter les d√©lais d'acc√®s.

#### Exemple : Analogie avec un Phare
Le phare r√©gule l'entr√©e des bateaux dans un port.
- **Vert (1)** : Acc√®s autoris√© √† un bateau.
- **Rouge (0)** : Attente obligatoire pour les suivants.
Cela illustre les concepts de `wait` (rouge) et `signal` (vert).

#### Proc√©d√©
Un s√©maphore unique contr√¥le l'acc√®s au deuxi√®me tiers d'une fen√™tre, divis√©e en six sections. Un thread appelle `sem.syncWait()` pour entrer dans la section critique et `sem.syncSignal()` en sortant, assurant qu'un seul mobile y passe √† la fois.

### Conclusion
Les s√©maphores simplifient la synchronisation des threads mais n√©cessitent une impl√©mentation soign√©e pour √©viter les interblocages et pr√©server les performances.

---

## TP3 : Bo√Æte aux lettres

### Analyse et R√©solution

La **BAL (Bo√Æte aux Lettres)** est une file circulaire utilis√©e pour un mod√®le producteur-consommateur. Elle s'appuie sur la classe **`BlockingQueue`** pour synchroniser automatiquement l'acc√®s.

Dans ce TP, j‚Äôai structur√© la solution autour de la classe **BAL**, qui encapsule l‚Äôinterface **`BlockingQueue`** et simplifie son utilisation. La **BAL** agit comme un point central de communication entre les threads producteurs et consommateurs. Elle masque les d√©tails complexes de synchronisation, en utilisant une instance concr√®te de **`ArrayBlockingQueue`** comme tampon circulaire.

Cette organisation permet une s√©paration claire des responsabilit√©s : les producteurs et consommateurs interagissent uniquement avec la **BAL** via des m√©thodes d√©di√©es (deposer et retirer), tandis que la gestion des acc√®s concurrents est enti√®rement confi√©e √† l‚Äôimpl√©mentation sous-jacente de la **`BlockingQueue`**. Cette approche favorise la modularit√©, la lisibilit√©, la portabilit√© et la maintenance du code.

### Ce que j'ai appris :
1. **Gestion simplifi√©e des threads** :
   Les m√©thodes comme `offer` (ajout) et `poll` (retrait) √©vitent les blocages ind√©finis.

2. **Arr√™t propre** :
   Un marqueur sp√©cial signale la fin de la production, permettant aux threads de se terminer correctement.

3. **Parall√®le avec la boulangerie** :
   - **D√©p√¥t** : Un boulanger place un pain dans un panier (lettre dans la BAL).
   - **Retrait** : Un client retire un pain, ou attend si le panier est vide.
   - **Marqueur de fermeture** : Indique la fin de l'activit√©.

### Conclusion

Ce TP illustre les avantages des outils de synchronisation modernes, comme **`BlockingQueue`**, pour g√©rer efficacement la concurrence, pr√©venir les blocages et simplifier le code.

---

## TP4 : Concepts avanc√©s et Monte Carlo

### Master / Worker
Le mod√®le **Master/Worker** est une architecture de programmation parall√®le o√π un **master** distribue des t√¢ches √† plusieurs **workers**. Le master coordonne les t√¢ches et collecte les r√©sultats, tandis que les workers ex√©cutent les t√¢ches en parall√®le.

### Future
Un **Future** repr√©sente le r√©sultat d'une op√©ration asynchrone. Il permet de v√©rifier si le calcul est termin√© et de r√©cup√©rer le r√©sultat une fois disponible.

### Acc√©l√©ration (Speedup)
L'**acc√©l√©ration** mesure le gain de performance obtenu en utilisant plusieurs processeurs par rapport √† un seul. Elle est calcul√©e comme le rapport entre le temps d'ex√©cution s√©quentiel et le temps d'ex√©cution parall√®le.

### Scalabilit√©
La **scalabilit√©** est la capacit√© d'un syst√®me √† augmenter ses performances proportionnellement √† l'ajout de ressources (comme des processeurs). Elle peut √™tre **forte** (augmentation lin√©aire) ou **faible** (augmentation sous-lin√©aire).

### Work Stealing Pool
Un **Work Stealing Pool** est une structure de donn√©es o√π les threads volent des t√¢ches √† d'autres threads lorsqu'ils sont inactifs, optimisant ainsi l'utilisation des ressources.

### Application de l'API Concurrent
L'API **Concurrent** de Java fournit des outils pour la programmation parall√®le, comme les **Executors**, les **Futures**, et les **Concurrent Collections**, facilitant la gestion des threads et des t√¢ches.

### M√©thode de Monte Carlo
La m√©thode de **Monte Carlo** est une technique statistique utilis√©e pour estimer des r√©sultats num√©riques en g√©n√©rant des √©chantillons al√©atoires. Elle est souvent utilis√©e pour des simulations complexes.

![illustration pi montecarlo](./assets/pi_montecarlo.png)

La parall√©lisation par t√¢che est une technique utilis√©e pour am√©liorer les performances d'un programme en ex√©cutant plusieurs t√¢ches simultan√©ment sur plusieurs processeurs ou c≈ìurs. Dans le contexte de la m√©thode de Monte Carlo, la parall√©lisation par t√¢che peut √™tre particuli√®rement efficace car cette m√©thode repose sur la g√©n√©ration d'un grand nombre d'√©chantillons al√©atoires ind√©pendants.

#### Explication de la Parall√©lisation par T√¢che

1. **D√©composition en T√¢ches** :
   - **T√¢che** : Une t√¢che est une unit√© de travail ind√©pendante qui peut √™tre ex√©cut√©e s√©par√©ment. Dans le cas de la m√©thode de Monte Carlo, chaque √©chantillon al√©atoire ou groupe d'√©chantillons peut √™tre consid√©r√© comme une t√¢che.
   - **D√©composition** : Le probl√®me global est divis√© en plusieurs t√¢ches plus petites. Par exemple, si vous devez g√©n√©rer 1 000 000 d'√©chantillons, vous pouvez diviser ce travail en 10 t√¢ches de 100 000 √©chantillons chacune.

2. **Ex√©cution Parall√®le** :
   - **Processeurs/C≈ìurs** : Chaque t√¢che est assign√©e √† un processeur ou c≈ìur diff√©rent pour √™tre ex√©cut√©e simultan√©ment. Si vous avez 4 c≈ìurs, vous pouvez ex√©cuter 4 t√¢ches en parall√®le.
   - **Ind√©pendance** : Les t√¢ches doivent √™tre ind√©pendantes les unes des autres pour √©viter les conflits d'acc√®s aux ressources partag√©es. Dans la m√©thode de Monte Carlo, les √©chantillons sont g√©n√©ralement ind√©pendants, ce qui facilite la parall√©lisation.

3. **Synchronisation et Agr√©gation** :
   - **Synchronisation** : Une fois que toutes les t√¢ches sont termin√©es, les r√©sultats doivent √™tre synchronis√©s. Cela peut impliquer la collecte des r√©sultats de chaque t√¢che et leur agr√©gation pour obtenir le r√©sultat final.
   - **Agr√©gation** : Les r√©sultats partiels des diff√©rentes t√¢ches sont combin√©s pour obtenir le r√©sultat global. Par exemple, si chaque t√¢che calcule une estimation partielle, ces estimations peuvent √™tre moyenn√©es pour obtenir l'estimation finale.

#### Exemple de Code Parall√©lis√©

Voici un exemple de parall√©lisation par t√¢che pour le calcul de PI avec la m√©thode de Monte Carlo, utilisant l'api concurrent :

```python
import concurrent.futures
import random

def monte_carlo_task(num_samples):
    inside_circle = 0
    for _ in range(num_samples):
        x, y = random.random(), random.random()
        if x**2 + y**2 <= 1:
            inside_circle += 1
    return inside_circle

def monte_carlo_pi(num_samples, num_tasks):
    samples_per_task = num_samples // num_tasks
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [executor.submit(monte_carlo_task, samples_per_task) for _ in range(num_tasks)]
        inside_circle_counts = [future.result() for future in concurrent.futures.as_completed(futures)]
    total_inside_circle = sum(inside_circle_counts)
    return (total_inside_circle / num_samples) * 4
```

#### Explication de la Parall√©lisation
**D√©composition en T√¢ches :**
- La fonction monte_carlo_task repr√©sente une t√¢che individuelle. Elle prend un nombre d'√©chantillons (num_samples) et compte combien de ces √©chantillons tombent √† l'int√©rieur d'un cercle unit√©.
- La fonction monte_carlo_pi divise le nombre total d'√©chantillons (num_samples) en plusieurs t√¢ches (num_tasks). Chaque t√¢che traite un sous-ensemble des √©chantillons.

**Ex√©cution Parall√®le :**
- ProcessPoolExecutor : La biblioth√®que concurrent.futures est utilis√©e pour cr√©er un pool de processus (ProcessPoolExecutor). Cela permet d'ex√©cuter plusieurs t√¢ches en parall√®le sur plusieurs processeurs ou c≈ìurs.
- submit : La m√©thode executor.submit soumet chaque t√¢che (monte_carlo_task) au pool de processus. Chaque t√¢che est ex√©cut√©e de mani√®re ind√©pendante et en parall√®le.
- futures : Les objets future repr√©sentent les t√¢ches en cours d'ex√©cution. Ils permettent de r√©cup√©rer les r√©sultats une fois que les t√¢ches sont termin√©es.

**Synchronisation et Agr√©gation :**
- as_completed : La m√©thode concurrent.futures.as_completed permet de r√©cup√©rer les r√©sultats des t√¢ches au fur et √† mesure qu'elles se terminent.
- result : La m√©thode future.result() r√©cup√®re le r√©sultat de chaque t√¢che. Les r√©sultats sont ensuite agr√©g√©s pour obtenir le nombre total de points √† l'int√©rieur du cercle.
- Calcul Final : Le r√©sultat final est calcul√© en utilisant la formule de Monte Carlo pour estimer œÄ.

#### Avantages de la Parall√©lisation par T√¢che

- **Performance** : R√©duit le temps d'ex√©cution en utilisant plusieurs processeurs ou c≈ìurs.
- **Scalabilit√©** : Peut √™tre facilement adapt√© pour utiliser plus de ressources mat√©rielles.
- **Simplicit√©** : Les t√¢ches ind√©pendantes sont plus faciles √† g√©rer et √† synchroniser.

#### Conclusion

La parall√©lisation par t√¢che est une technique puissante pour am√©liorer les performances des simulations de Monte Carlo en tirant parti des architectures multi-c≈ìurs modernes. En d√©composant le probl√®me en t√¢ches ind√©pendantes et en les ex√©cutant en parall√®le, on peut obtenir des r√©sultats plus rapidement et de mani√®re plus efficace.


### Analyse des performances de Monte Carlo
La m√©thode de Monte Carlo peut b√©n√©ficier d'une bonne scalabilit√© gr√¢ce √† son parall√©lisme naturel. Cependant, les limitations num√©riques et mat√©rielles peuvent rapidement limiter les gains de performance.


---

## Programmation distribu√©e

### Master / Worker en distribu√©
Dans un environnement distribu√©, le **master** envoie des messages aux **workers** via un r√©seau. Les workers ex√©cutent les t√¢ches en parall√®le et renvoient les r√©sultats au master.

### Analyse des Sockets JAVA

![Image UML WorkerSocket](./assets/Socket%20rapport%20TP.jpg)

#### Analyse MasterSocket.java

**√âtapes principales dans le code**
**Initialisation des workers** :
- Le master demande combien de workers (processus) seront utilis√©s. Il ouvre un socket (canal de communication) pour chaque worker sur un port donn√©.
**Envoi des t√¢ches aux workers** :
- Chaque worker re√ßoit le nombre total de points √† g√©n√©rer pour l'estimation de ùúã.

**Traitement par les workers** :
- Les workers g√©n√®rent des points al√©atoires dans un carr√©, comptent ceux qui tombent dans un quart de cercle et renvoient leurs r√©sultats au master.

**R√©cup√©ration des r√©sultats** :
- Le master collecte les r√©sultats des workers via leurs sockets respectifs.
- Il combine ces r√©sultats pour calculer la valeur approximative de ùúã.

**Affichage des r√©sultats** :
- Le master affiche ùúã, l'erreur relative, et les statistiques de performance (dur√©e, nombre de points, etc.).
- L'utilisateur peut choisir de r√©p√©ter la simulation.

**Fermeture des sockets** :
- Une fois la simulation termin√©e, les sockets entre le master et les workers sont ferm√©s proprement.

**Sockets :**
- **Socket c√¥t√© master** : Utilis√© pour envoyer des t√¢ches et recevoir des r√©sultats.
- **Socket c√¥t√© worker (non montr√© ici)** : √âcoute les messages du master, ex√©cute la t√¢che, puis renvoie le r√©sultat.

**Flux g√©n√©ral de la communication**
**Master :**
- Connecte un socket pour chaque worker.
- Envoie une t√¢che √† chaque worker via un flux d'√©criture (PrintWriter).
- Lit les r√©sultats des workers via un flux de lecture (BufferedReader).

**Workers (c√¥t√© serveur, non montr√© ici) :**
- √âcoute sur un port sp√©cifique.
- Re√ßoit les donn√©es envoy√©es par le master.
- Effectue le calcul et renvoie le r√©sultat.

#### Analyse WorkerSocket.java

**√âtapes principales du code Worker**
**Configuration du Worker :**
- Le Worker d√©marre un serveur socket sur un port donn√© (par d√©faut 25545 ou sp√©cifi√© en argument).
- Il attend une connexion entrante du Master via le socket.

**Communication avec le Master :**
- Une fois connect√©, le Worker √©coute les messages du Master en utilisant un flux d'entr√©e (BufferedReader).
- Apr√®s avoir re√ßu les donn√©es, il effectue le calcul n√©cessaire.

**Envoi des r√©sultats au Master :**
- Le Worker utilise un flux de sortie (PrintWriter) pour renvoyer les r√©sultats du calcul au Master.

**Gestion des messages et fin de t√¢che :**
- Si le message re√ßu est "END", le Worker arr√™te son ex√©cution.
Sinon, il continue √† traiter les donn√©es envoy√©es par le Master.

**Flux de communication entre Master et Worker**
**C√¥t√© Master :**
- **Client socket :** Le Master initie la connexion et envoie des t√¢ches aux Workers. Le Master attend les r√©ponses des Workers pour agr√©ger les r√©sultats.

**C√¥t√© Worker :**
- **Serveur socket :** Le Worker accepte les connexions du Master. Chaque message re√ßu est interpr√©t√© comme une instruction (par exemple, combien de points g√©n√©rer pour la m√©thode Monte Carlo). Une fois le calcul termin√©, le r√©sultat est renvoy√© au Master.

### Monte Carlo Master/Worker Socket

Pour calculer œÄ et √©tablir les communications Master/Worker, les changements suivants ont √©t√© apport√©s :
- Le **Master** distribue le nombre total de points √† g√©n√©rer aux **Workers**.
- Chaque **Worker** g√©n√®re des points al√©atoires et compte ceux qui tombent dans un quart de cercle.
- Les r√©sultats sont renvoy√©s au **Master**, qui les agr√®ge pour estimer œÄ.

### √âtude de la Scalabilit√©

Pour √©tudier la scalabilit√©, nous avons choisi un nombre d'essais suffisamment grand pour mesurer le temps d'ex√©cution de mani√®re significative. Nous avons test√© avec plusieurs workers et compar√© les r√©sultats.

#### Pi.java

Un script a √©t√© mis en place pour r√©aliser un graphique de la scalabilit√© forte de Pi.java, en modifiant le nombre de workers. Nous avons observ√© qu'au-del√† de 8 c≈ìurs, le speedup diminue en raison des limitations mat√©rielles de la machine.

- **Parall√©lisation bas√©e sur un pool de threads fixe :**
  Le programme utilise `Executors.newFixedThreadPool(numWorkers)` pour cr√©er un pool de `numWorkers` threads, ce qui permet un contr√¥le efficace du parall√©lisme en limitant le nombre de threads actifs.

- **R√©partition des t√¢ches :**
  Chaque worker (`Callable<Long>`) ex√©cute une fraction du nombre total d‚Äôit√©rations et renvoie un r√©sultat partiel. L‚Äôagr√©gation des r√©sultats se fait via `Future.get()`, qui agit comme une **barri√®re implicite** (les r√©sultats sont collect√©s une fois que tous les threads ont termin√©).

- **Avantages de cette approche :**
  - Les t√¢ches sont bien √©quilibr√©es entre les threads (`totalCount / numWorkers` par worker).
  - L'utilisation de `invokeAll()` garantit que tous les threads travaillent en parall√®le avant de r√©cup√©rer les r√©sultats.
  - Contrairement √† `Assignment102.java`, le programme **r√©duit le nombre de t√¢ches** en regroupant plusieurs it√©rations par worker, √©vitant ainsi la surcharge li√©e √† la gestion d‚Äôun trop grand nombre de petites t√¢ches.

- **Limites observ√©es :**
  - Au-del√† de 8 threads, le **speedup diminue** en raison de la saturation des ressources CPU et des co√ªts de synchronisation.
  - L‚Äôappel `Future.get()` bloque le thread principal jusqu'√† ce que tous les workers aient termin√©, ce qui peut cr√©er un goulot d‚Äô√©tranglement si certains threads prennent plus de temps que d‚Äôautres.
  - L'utilisation de `Random` par chaque thread peut provoquer une contention si plusieurs threads g√©n√®rent des nombres al√©atoires en parall√®le.

#### Assignments102

Le script pr√©c√©dent a √©t√© r√©utilis√© pour `Assignments102.java`. Ce paradigme fonctionne diff√©remment et les mesures r√©v√®lent qu'il est moins efficace. Le speedup de la scalabilit√© forte du code `Assignments102` est proche de la scalabilit√© faible de `Pi.java`.

- **Trop de t√¢ches cr√©√©es (1 t√¢che par point simul√©) :**
  Chaque simulation de point (x, y) est soumise individuellement au pool de threads, ce qui entra√Æne une surcharge importante sur le gestionnaire de t√¢ches. La cr√©ation et la gestion d'un grand nombre de threads tr√®s courts g√©n√®rent un overhead significatif et r√©duisent l'efficacit√© globale du programme.

- **Probl√®me de synchronisation sur `AtomicInteger` :**
  L'utilisation de `AtomicInteger.incrementAndGet()` pour compter les points dans le cercle cr√©e une contention m√©moire entre threads. Chaque mise √† jour √©tant atomique, cela ralentit l'acc√®s concurrentiel. Une alternative plus performante serait l'utilisation de `LongAdder`, qui r√©duit les conflits en r√©partissant les mises √† jour sur plusieurs variables internes.

Globalement, `Pi.java` est mieux parall√©lis√© que `Assignment102.java`, car il minimise la surcharge de cr√©ation de threads et utilise un **mod√®le de travail bien √©quilibr√©** pour exploiter efficacement les ressources CPU.

### Comparaison Assignment102 vs Pi

#### Diff√©rences d'erreurs
L'erreur dans l'estimation de œÄ varie en fonction de la m√©thode utilis√©e :
- **`Assignment102`** : Chaque point g√©n√©r√© est trait√© individuellement, ce qui entra√Æne une grande variabilit√© due aux fluctuations al√©atoires et aux conflits d'acc√®s sur une variable partag√©e. La convergence est plus lente et l'erreur plus importante.
- **`Pi.java`** : La g√©n√©ration de points est r√©partie en lots et ex√©cut√©e en parall√®le, r√©duisant l'impact des erreurs al√©atoires et am√©liorant la stabilit√© de l'estimation de œÄ.

Une analyse des r√©sultats montre que `Pi.java` produit des valeurs plus pr√©cises avec une variance plus faible compar√©e √† `Assignment102`.

#### Diff√©rences de paradigmes de programmation

1. **Approche `Assignment102` : Parall√©lisme de fine granularit√©**
   - Chaque point al√©atoire est trait√© comme une t√¢che distincte.
   - Utilisation intensive de `AtomicInteger` pour la synchronisation, entra√Ænant des ralentissements dus √† la contention m√©moire.
   - Nombre tr√®s √©lev√© de t√¢ches l√©g√®res soumises √† un gestionnaire de threads, augmentant la surcharge du syst√®me.

2. **Approche `Pi.java` : Parall√©lisme par regroupement de t√¢ches**
   - Chaque thread traite un bloc d'√©chantillons, r√©duisant le nombre de t√¢ches cr√©√©es.
   - Synchronisation minimale gr√¢ce √† l'agr√©gation locale avant soumission des r√©sultats.
   - Utilisation optimis√©e des c≈ìurs processeur avec un pool de threads √©quilibr√©.

#### Impact sur la Scalabilit√©
- **`Assignment102`** pr√©sente une scalabilit√© limit√©e en raison du nombre excessif de t√¢ches et de la contention sur `AtomicInteger`.
- **`Pi.java`** est plus efficace car il minimise les conflits et optimise l'utilisation des ressources CPU.

En conclusion, la m√©thode `Pi.java` surpasse `Assignment102` en termes d'efficacit√© et de pr√©cision gr√¢ce √† une meilleure gestion des t√¢ches et une r√©duction des conflits d'acc√®s concurrentiels.



TODO : faire un graphe d'erreur TP / P et scal faible assignments102