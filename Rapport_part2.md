# Rapport G√©n√©ral - Partie 2

>[Retour au README](./README.md)

## Table des mati√®res

- [Rapport G√©n√©ral - Partie 2](#rapport-g√©n√©ral---partie-2)
  - [Table des mati√®res](#table-des-mati√®res)
  - [TP4 : Concepts avanc√©s et Monte Carlo](#tp4--concepts-avanc√©s-et-monte-carlo)
  - [Objectif](#objectif)
    - [Master / Worker](#master--worker)
    - [Future](#future)
    - [Acc√©l√©ration (Speedup)](#acc√©l√©ration-speedup)
    - [Scalabilit√©](#scalabilit√©)
    - [Work Stealing Pool](#work-stealing-pool)
    - [Application de l'API Concurrent](#application-de-lapi-concurrent)
    - [M√©thode de Monte Carlo](#m√©thode-de-monte-carlo)
      - [Explication de la Parall√©lisation par T√¢che](#explication-de-la-parall√©lisation-par-t√¢che)
      - [Exemple de Code Parall√©lis√©](#exemple-de-code-parall√©lis√©)
      - [Explication de la Parall√©lisation](#explication-de-la-parall√©lisation)
      - [Avantages de la Parall√©lisation par T√¢che](#avantages-de-la-parall√©lisation-par-t√¢che)
      - [Conclusion](#conclusion)
    - [Analyse des performances de Monte Carlo](#analyse-des-performances-de-monte-carlo)
    - [√âtude de la Scalabilit√©](#√©tude-de-la-scalabilit√©)
      - [Pi.java](#pijava)
      - [Assignments102](#assignments102)
    - [Comparaison Assignment102 vs Pi](#comparaison-assignment102-vs-pi)
      - [Diff√©rences d'erreurs](#diff√©rences-derreurs)
      - [Diff√©rences de paradigmes de programmation](#diff√©rences-de-paradigmes-de-programmation)
      - [Impact sur la Scalabilit√©](#impact-sur-la-scalabilit√©)
  - [Programmation distribu√©e](#programmation-distribu√©e)
    - [Master / Worker en distribu√©](#master--worker-en-distribu√©)
    - [Analyse des Sockets JAVA](#analyse-des-sockets-java)
    - [Monte Carlo Master/Worker Socket](#monte-carlo-masterworker-socket)
      - [Analyse MasterSocket.java](#analyse-mastersocketjava)
      - [Analyse WorkerSocket.java](#analyse-workersocketjava)
    - [Mise en place des mesures en architecture distribu√©e](#mise-en-place-des-mesures-en-architecture-distribu√©e)
  - [Qualit√© du code](#qualit√©-du-code)
    - [Quality in Use](#quality-in-use)
      - [1. Effectiveness (Efficacit√©)](#1-effectiveness-efficacit√©)
      - [2. Efficiency (Efficience)](#2-efficiency-efficience)
      - [3. Satisfaction](#3-satisfaction)
      - [4. Freedom from Risk (R√©duction des risques)](#4-freedom-from-risk-r√©duction-des-risques)
      - [5. Context Coverage (Couverture du contexte)](#5-context-coverage-couverture-du-contexte)
    - [Product Quality](#product-quality)
      - [1. Fonctionnalit√©](#1-fonctionnalit√©)
      - [2. Fiabilit√©](#2-fiabilit√©)
      - [3. Performance et Efficience](#3-performance-et-efficience)
      - [4. Maintenabilit√©](#4-maintenabilit√©)
      - [5. S√©curit√©](#5-s√©curit√©)
  - [R√©f√©rences et sources](#r√©f√©rences-et-sources)

## TP4 : Concepts avanc√©s et Monte Carlo

## Objectif
L'objectif de ce TP est d'explorer diff√©rentes approches pour calculer œÄ en utilisant la m√©thode de Monte Carlo. En comparant les paradigmes de programmation de `Pi.java` et `Assignment102`, nous examinons les performances et l'efficacit√© de la programmation √† m√©moire partag√©e et distribu√©e. Cette √©tude vise √† comprendre comment la parall√©lisation et la distribution des t√¢ches peuvent optimiser les calculs intensifs, tout en mettant en avant les contraintes li√©es √† la synchronisation et √† la gestion des ressources.

### Master / Worker
Le mod√®le **Master/Worker** est une architecture de programmation parall√®le o√π un **master** distribue des t√¢ches √† plusieurs **workers**. Le master coordonne les t√¢ches et collecte les r√©sultats, tandis que les workers ex√©cutent les t√¢ches en parall√®le.

### Future
Un **Future** repr√©sente le r√©sultat d'une op√©ration asynchrone. Il permet de v√©rifier si le calcul est termin√© et de r√©cup√©rer le r√©sultat une fois disponible.

### Acc√©l√©ration (Speedup)
L'**acc√©l√©ration** mesure le gain de performance obtenu en utilisant plusieurs processeurs par rapport √† un seul. Elle est calcul√©e comme le rapport entre le temps d'ex√©cution s√©quentiel et le temps d'ex√©cution parall√®le.

### Scalabilit√©
La **scalabilit√©** est la capacit√© d'un syst√®me √† augmenter ses performances proportionnellement √† l'ajout de ressources (comme des processeurs). Elle peut √™tre **forte** (augmentation lin√©aire) ou **faible** (augmentation sous-lin√©aire).

### Work Stealing Pool
Un **Work Stealing Pool** est une structure de donn√©es o√π les threads volent des t√¢ches √† d'autres threads lorsqu'ils sont inactifs, optimisant ainsi l'utilisation des ressources.

### Application de l'API Concurrent
L'API **Concurrent** de Java fournit des outils pour la programmation parall√®le, comme les **Executors**, les **Futures**, et les **Concurrent Collections**, facilitant la gestion des threads et des t√¢ches.

### M√©thode de Monte Carlo
La m√©thode de **Monte Carlo** est une technique statistique utilis√©e pour estimer des r√©sultats num√©riques en g√©n√©rant des √©chantillons al√©atoires. Elle est souvent utilis√©e pour des simulations complexes.

![illustration pi montecarlo](./assets/pi_montecarlo.png)

La parall√©lisation par t√¢che est une technique utilis√©e pour am√©liorer les performances d'un programme en ex√©cutant plusieurs t√¢ches simultan√©ment sur plusieurs processeurs ou c≈ìurs. Dans le contexte de la m√©thode de Monte Carlo, la parall√©lisation par t√¢che peut √™tre particuli√®rement efficace car cette m√©thode repose sur la g√©n√©ration d'un grand nombre d'√©chantillons al√©atoires ind√©pendants.

#### Explication de la Parall√©lisation par T√¢che

1. **D√©composition en T√¢ches** :
   - **T√¢che** : Une t√¢che est une unit√© de travail ind√©pendante qui peut √™tre ex√©cut√©e s√©par√©ment. Dans le cas de la m√©thode de Monte Carlo, chaque √©chantillon al√©atoire ou groupe d'√©chantillons peut √™tre consid√©r√© comme une t√¢che.
   - **D√©composition** : Le probl√®me global est divis√© en plusieurs t√¢ches plus petites. Par exemple, si vous devez g√©n√©rer 1 000 000 d'√©chantillons, vous pouvez diviser ce travail en 10 t√¢ches de 100 000 √©chantillons chacune.

2. **Ex√©cution Parall√®le** :
   - **Processeurs/C≈ìurs** : Chaque t√¢che est assign√©e √† un processeur ou c≈ìur diff√©rent pour √™tre ex√©cut√©e simultan√©ment. Si vous avez 4 c≈ìurs, vous pouvez ex√©cuter 4 t√¢ches en parall√®le.
   - **Ind√©pendance** : Les t√¢ches doivent √™tre ind√©pendantes les unes des autres pour √©viter les conflits d'acc√®s aux ressources partag√©es. Dans la m√©thode de Monte Carlo, les √©chantillons sont g√©n√©ralement ind√©pendants, ce qui facilite la parall√©lisation.

3. **Synchronisation et Agr√©gation** :
   - **Synchronisation** : Une fois que toutes les t√¢ches sont termin√©es, les r√©sultats doivent √™tre synchronis√©s. Cela peut impliquer la collecte des r√©sultats de chaque t√¢che et leur agr√©gation pour obtenir le r√©sultat final.
   - **Agr√©gation** : Les r√©sultats partiels des diff√©rentes t√¢ches sont combin√©s pour obtenir le r√©sultat global. Par exemple, si chaque t√¢che calcule une estimation partielle, ces estimations peuvent √™tre moyenn√©es pour obtenir l'estimation finale.

#### Exemple de Code Parall√©lis√©

Voici un exemple de parall√©lisation par t√¢che pour le calcul de PI avec la m√©thode de Monte Carlo, utilisant l'API concurrent :

```python
import concurrent.futures
import random

def monte_carlo_task(num_samples):
    inside_circle = 0
    for _ in range(num_samples):
        x, y = random.random(), random.random()
        if x**2 + y**2 <= 1:
            inside_circle += 1
    return inside_circle

def monte_carlo_pi(num_samples, num_tasks):
    samples_per_task = num_samples // num_tasks
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [executor.submit(monte_carlo_task, samples_per_task) for _ in range(num_tasks)]
        inside_circle_counts = [future.result() for future in concurrent.futures.as_completed(futures)]
    total_inside_circle = sum(inside_circle_counts)
    return (total_inside_circle / num_samples) * 4
```

#### Explication de la Parall√©lisation
**D√©composition en T√¢ches :**
- La fonction `monte_carlo_task` repr√©sente une t√¢che individuelle. Elle prend un nombre d'√©chantillons (`num_samples`) et compte combien de ces √©chantillons tombent √† l'int√©rieur d'un cercle unit√©.
- La fonction `monte_carlo_pi` divise le nombre total d'√©chantillons (`num_samples`) en plusieurs t√¢ches (`num_tasks`). Chaque t√¢che traite un sous-ensemble des √©chantillons.

**Ex√©cution Parall√®le :**
- **ProcessPoolExecutor** : La biblioth√®que `concurrent.futures` est utilis√©e pour cr√©er un pool de processus (`ProcessPoolExecutor`). Cela permet d'ex√©cuter plusieurs t√¢ches en parall√®le sur plusieurs processeurs ou c≈ìurs.
- **submit** : La m√©thode `executor.submit` soumet chaque t√¢che (`monte_carlo_task`) au pool de processus. Chaque t√¢che est ex√©cut√©e de mani√®re ind√©pendante et en parall√®le.
- **futures** : Les objets `future` repr√©sentent les t√¢ches en cours d'ex√©cution. Ils permettent de r√©cup√©rer les r√©sultats une fois que les t√¢ches sont termin√©es.

**Synchronisation et Agr√©gation :**
- **as_completed** : La m√©thode `concurrent.futures.as_completed` permet de r√©cup√©rer les r√©sultats des t√¢ches au fur et √† mesure qu'elles se terminent.
- **result** : La m√©thode `future.result()` r√©cup√®re le r√©sultat de chaque t√¢che. Les r√©sultats sont ensuite agr√©g√©s pour obtenir le nombre total de points √† l'int√©rieur du cercle.
- **Calcul Final** : Le r√©sultat final est calcul√© en utilisant la formule de Monte Carlo pour estimer œÄ.

#### Avantages de la Parall√©lisation par T√¢che

- **Performance** : R√©duit le temps d'ex√©cution en utilisant plusieurs processeurs ou c≈ìurs.
- **Scalabilit√©** : Peut √™tre facilement adapt√© pour utiliser plus de ressources mat√©rielles.
- **Simplicit√©** : Les t√¢ches ind√©pendantes sont plus faciles √† g√©rer et √† synchroniser.

#### Conclusion

La parall√©lisation par t√¢che est une technique puissante pour am√©liorer les performances des simulations de Monte Carlo en tirant parti des architectures multi-c≈ìurs modernes. En d√©composant le probl√®me en t√¢ches ind√©pendantes et en les ex√©cutant en parall√®le, on peut obtenir des r√©sultats plus rapidement et de mani√®re plus efficace.

### Analyse des performances de Monte Carlo
La m√©thode de Monte Carlo peut b√©n√©ficier d'une bonne scalabilit√© gr√¢ce √† son parall√©lisme naturel. Cependant, les limitations num√©riques et mat√©rielles peuvent rapidement limiter les gains de performance.

### √âtude de la Scalabilit√©

Pour √©tudier la scalabilit√©, nous avons choisi un nombre d'essais suffisamment grand pour mesurer le temps d'ex√©cution de mani√®re significative. Nous avons test√© avec plusieurs workers et compar√© les r√©sultats. Dans ce cas il s'agissait de 12 millions d'op√©rations.

En raison de plusieurs contraintes, toutes les mesures souhait√©es n'ont pas pu √™tre r√©alis√©e comme pr√©vue. Cependant il y en a assez pour illustrer les propos abord√©s dans cette partie.

#### Pi.java

Un script a √©t√© mis en place pour r√©aliser un graphique de la scalabilit√© forte de Pi.java, en modifiant le nombre de workers. Nous avons observ√© qu'au-del√† de 8 c≈ìurs, le speedup diminue en raison des limitations mat√©rielles de la machine.

**Scalabilit√© forte sur une machine en G26**
![Scalabilit√© forte Pi G26](./TP4/results_scalabilite/scal_forte_G26_pi.png)

**Scalabilit√© forte sur mon MacBook M1 pro**
![Scalabilit√© forte Pi Mac](./TP4/results_scalabilite/scal_forte_mac_pi.png)

**Scalabilit√© faible sur une machine en G26**
![Scalabilit√© faible Pi G26](./TP4/results_scalabilite/scal_faible_G26_pi.png)

**Scalabilit√© faible sur mon MacBook M1 pro**
![Scalabilit√© faible Pi Mac](./TP4/results_scalabilite/scal_faible_mac_pi.png)

- **Parall√©lisation bas√©e sur un pool de threads fixe :**
  Le programme utilise `Executors.newFixedThreadPool(numWorkers)` pour cr√©er un pool de `numWorkers` threads, ce qui permet un contr√¥le efficace du parall√©lisme en limitant le nombre de threads actifs.

- **R√©partition des t√¢ches :**
  Chaque worker (`Callable<Long>`) ex√©cute une fraction du nombre total d‚Äôit√©rations et renvoie un r√©sultat partiel. L‚Äôagr√©gation des r√©sultats se fait via `Future.get()`, qui agit comme une **barri√®re implicite** (les r√©sultats sont collect√©s une fois que tous les threads ont termin√©).

- **Avantages de cette approche :**
  - Les t√¢ches sont bien √©quilibr√©es entre les threads (`totalCount / numWorkers` par worker).
  - L'utilisation de `invokeAll()` garantit que tous les threads travaillent en parall√®le avant de r√©cup√©rer les r√©sultats.
  - Contrairement √† `Assignment102.java`, le programme **r√©duit le nombre de t√¢ches** en regroupant plusieurs it√©rations par worker, √©vitant ainsi la surcharge li√©e √† la gestion d‚Äôun trop grand nombre de petites t√¢ches.

- **Limites observ√©es :**
  - Au-del√† de 8 threads, le **speedup diminue** en raison de la saturation des ressources CPU et des co√ªts de synchronisation.
  - L‚Äôappel `Future.get()` bloque le thread principal jusqu'√† ce que tous les workers aient termin√©, ce qui peut cr√©er un goulot d‚Äô√©tranglement si certains threads prennent plus de temps que d‚Äôautres.
  - L'utilisation de `Random` par chaque thread peut provoquer une contention si plusieurs threads g√©n√®rent des nombres al√©atoires en parall√®le.

#### Assignments102

Le script pr√©c√©dent a √©t√© r√©utilis√© pour `Assignments102.java`. Ce paradigme fonctionne diff√©remment et les mesures r√©v√®lent qu'il est moins efficace. Le speedup de la scalabilit√© forte du code `Assignments102` est proche de la scalabilit√© faible de `Pi.java`.

**Scalabilit√© forte sur mon MacBook M1 pro**
![Scalabilit√© forte Assignments102 Mac](./TP4/results_scalabilite/scal_forte_mac_assignments102.png)

**Scalabilit√© faible attendue**
![Scalabilit√© faible Assignments102 Mac](./TP4/results_scalabilite/scal_faible_assignemnts102.png)

- **Trop de t√¢ches cr√©√©es (1 t√¢che par point simul√©) :**
  Chaque simulation de point (x, y) est soumise individuellement au pool de threads, ce qui entra√Æne une surcharge importante sur le gestionnaire de t√¢ches. La cr√©ation et la gestion d'un grand nombre de threads tr√®s courts g√©n√®rent un overhead significatif et r√©duisent l'efficacit√© globale du programme.

- **Probl√®me de synchronisation sur `AtomicInteger` :**
  L'utilisation de `AtomicInteger.incrementAndGet()` pour compter les points dans le cercle cr√©e une contention m√©moire entre threads. Chaque mise √† jour √©tant atomique, cela ralentit l'acc√®s concurrentiel. Une alternative plus performante serait l'utilisation de `LongAdder`, qui r√©duit les conflits en r√©partissant les mises √† jour sur plusieurs variables internes.

Globalement, `Pi.java` est mieux parall√©lis√© que `Assignment102.java`, car il minimise la surcharge de cr√©ation de threads et utilise un **mod√®le de travail bien √©quilibr√©** pour exploiter efficacement les ressources CPU.

### Comparaison Assignment102 vs Pi

#### Diff√©rences d'erreurs

Malgr√®s le fait que je n'ai pas gard√©es mes mesures pour effectuer des graphiques voici ce que j'ai retenu des erreurs li√©es aux algorithmes:

L'erreur dans l'estimation de œÄ varie en fonction de la m√©thode utilis√©e :
- **`Assignment102`** : Chaque point g√©n√©r√© est trait√© individuellement, ce qui entra√Æne une grande variabilit√© due aux fluctuations al√©atoires et aux conflits d'acc√®s sur une variable partag√©e. La convergence est plus lente et l'erreur plus importante.
- **`Pi.java`** : La g√©n√©ration de points est r√©partie en lots et ex√©cut√©e en parall√®le, r√©duisant l'impact des erreurs al√©atoires et am√©liorant la stabilit√© de l'estimation de œÄ.

Cependant on remarque que les deux algorithmes montrent la m√™me tendance g√©n√©rale et poss√®dent une fiabilit√© √©quivalente.

On remarque par ailleurs que l'erreur diminue syst√©matiquement lorsque le nombre de points augmente.  

L'√©volution des erreurs montre que les deux algorithmes convergent vers 1 / racine(N), typique de Monte Carlo : plus on ajoute de points, plus l'erreur se r√©duit, de plus en plus lentement.


#### Diff√©rences de paradigmes de programmation

1. **Approche `Assignment102` : Parall√©lisme de fine granularit√©**
   - Chaque point al√©atoire est trait√© comme une t√¢che distincte.
   - Utilisation intensive de `AtomicInteger` pour la synchronisation, entra√Ænant des ralentissements dus √† la contention m√©moire.
   - Nombre tr√®s √©lev√© de t√¢ches l√©g√®res soumises √† un gestionnaire de threads, augmentant la surcharge du syst√®me.

2. **Approche `Pi.java` : Parall√©lisme par regroupement de t√¢ches**
   - Chaque thread traite un bloc d'√©chantillons, r√©duisant le nombre de t√¢ches cr√©√©es.
   - Synchronisation minimale gr√¢ce √† l'agr√©gation locale avant soumission des r√©sultats.
   - Utilisation optimis√©e des c≈ìurs processeur avec un pool de threads √©quilibr√©.

#### Impact sur la Scalabilit√©
- **`Assignment102`** pr√©sente une scalabilit√© limit√©e en raison du nombre excessif de t√¢ches et de la contention sur `AtomicInteger`.
- **`Pi.java`** est plus efficace car il minimise les conflits et optimise l'utilisation des ressources CPU.

Lors de la comparaison entre `Pi.java` et `Assignment102`, il est clair que `Pi.java` est plus efficace. `Pi.java` g√®re mieux les t√¢ches et √©vite les conflits entre les diff√©rentes parties du programme. Pour des projets o√π il faut bien utiliser les ressources √† disposition et obtenir des r√©sultats pr√©cis. Cette m√©thode simplifie la gestion des t√¢ches et assure une bonne coordination, ce qui est tr√®s utile quand les ressources sont limit√©es.  
En conclusion, la m√©thode `Pi.java` surpasse `Assignment102` en termes d'efficacit√© et de pr√©cision gr√¢ce √† une meilleure gestion des t√¢ches et une r√©duction des conflits d'acc√®s concurrentiels. 

## Programmation distribu√©e

### Master / Worker en distribu√©
Dans un environnement distribu√©, le **master** envoie des messages aux **workers** via un r√©seau. Les workers ex√©cutent les t√¢ches en parall√®le et renvoient les r√©sultats au master.

### Analyse des Sockets JAVA

![Image UML WorkerSocket](./assets/MW_Socket_UML.png)

### Monte Carlo Master/Worker Socket

Pour calculer œÄ et √©tablir les communications Master/Worker, les changements suivants ont √©t√© apport√©s :
- Le **Master** distribue le nombre total de points √† g√©n√©rer aux **Workers**.
- Chaque **Worker** g√©n√®re des points al√©atoires et compte ceux qui tombent dans un quart de cercle.
- Les r√©sultats sont renvoy√©s au **Master**, qui les agr√®ge pour estimer œÄ.

#### Analyse MasterSocket.java

**√âtapes principales dans le code**
**Initialisation des workers** :
- Le master demande combien de workers (processus) seront utilis√©s. Il ouvre un socket (canal de communication) pour chaque worker sur un port donn√©.
**Envoi des t√¢ches aux workers** :
- Chaque worker re√ßoit le nombre total de points √† g√©n√©rer pour l'estimation de ùúã.

**Traitement par les workers** :
- Les workers g√©n√®rent des points al√©atoires dans un carr√©, comptent ceux qui tombent dans un quart de cercle et renvoient leurs r√©sultats au master.

**R√©cup√©ration des r√©sultats** :
- Le master collecte les r√©sultats des workers via leurs sockets respectifs.
- Il combine ces r√©sultats pour calculer la valeur approximative de ùúã.

**Affichage des r√©sultats** :
- Le master affiche ùúã, l'erreur relative, et les statistiques de performance (dur√©e, nombre de points, etc.).
- L'utilisateur peut choisir de r√©p√©ter la simulation.

**Fermeture des sockets** :
- Une fois la simulation termin√©e, les sockets entre le master et les workers sont ferm√©s proprement.

**Sockets :**
- **Socket c√¥t√© master** : Utilis√© pour envoyer des t√¢ches et recevoir des r√©sultats.
- **Socket c√¥t√© worker (non montr√© ici)** : √âcoute les messages du master, ex√©cute la t√¢che, puis renvoie le r√©sultat.

**Flux g√©n√©ral de la communication**
**Master :**
- Connecte un socket pour chaque worker.
- Envoie une t√¢che √† chaque worker via un flux d'√©criture (PrintWriter).
- Lit les r√©sultats des workers via un flux de lecture (BufferedReader).

**Workers (c√¥t√© serveur, non montr√© ici) :**
- √âcoute sur un port sp√©cifique.
- Re√ßoit les donn√©es envoy√©es par le master.
- Effectue le calcul et renvoie le r√©sultat.

#### Analyse WorkerSocket.java

**√âtapes principales du code Worker**
**Configuration du Worker :**
- Le Worker d√©marre un serveur socket sur un port donn√© (par d√©faut 25545 ou sp√©cifi√© en argument).
- Il attend une connexion entrante du Master via le socket.

**Communication avec le Master :**
- Une fois connect√©, le Worker √©coute les messages du Master en utilisant un flux d'entr√©e (BufferedReader).
- Apr√®s avoir re√ßu les donn√©es, il effectue le calcul n√©cessaire.

**Envoi des r√©sultats au Master :**
- Le Worker utilise un flux de sortie (PrintWriter) pour renvoyer les r√©sultats du calcul au Master.

**Gestion des messages et fin de t√¢che :**
- Si le message re√ßu est "END", le Worker arr√™te son ex√©cution.
- Sinon, il continue √† traiter les donn√©es envoy√©es par le Master.

**Flux de communication entre Master et Worker**
**C√¥t√© Master :**
- **Client socket :** Le Master initie la connexion et envoie des t√¢ches aux Workers. Le Master attend les r√©ponses des Workers pour agr√©ger les r√©sultats.

**C√¥t√© Worker :**
- **Serveur socket :** Le Worker accepte les connexions du Master. Chaque message re√ßu est interpr√©t√© comme une instruction (par exemple, combien de points g√©n√©rer pour la m√©thode Monte Carlo). Une fois le calcul termin√©, le r√©sultat est renvoy√© au Master.

### Mise en place des mesures en architecture distribu√©e

Pour mettre en place l'architecture distribu√©e nous devons mettre en communs les adresses IP des machines qui vont servir aux machines √† communniquer.  

Ensuite il a fallut adapter le fichier Pi.java et le diviser en faisant un fichier par class afin de permettre une compilation sans erreurs.

**Calculs**  
Nous avons un Master qui envoie des requetes √† une dizaine de worker.

Chaque worker est une machine ind√©pendante et devra effectu√©e un total de 2 milliards d'op√©rations.

Nous utilisons Pi.java en scalabilit√© faible  pour acalculer un total de 64 milliards de points.

Pour un calcul de 2 milliard de points sur 16 machines avec 4 workers chacunes, nous arrivons √† une moyenne de 70 secondes avec 16, 32 et 64 processeurs. On bserve donc une tr√®s bonne scalabilit√© forte.

---
## Qualit√© du code  
La qualit√© du code est un √©l√©ment essentiel pour garantir la robustesse, la maintenabilit√© et l‚Äôefficacit√© d‚Äôun projet logiciel. 

La norme **ISO 25010** fait partie de la famille **SQuaRE (Software Quality Requirements and Evaluation)**, un ensemble de normes d√©finissant des mod√®les de qualit√© pour les logiciels et les syst√®mes informatiques. Son objectif est d‚Äôaider √† √©valuer, am√©liorer et garantir la qualit√© des logiciels en fournissant des crit√®res pr√©cis pour l‚Äôanalyse des performances, de la fiabilit√©, de la s√©curit√© et de l'exp√©rience utilisateur.

Selon ces normes, la qualit√© d‚Äôun logiciel est √©valu√©e selon plusieurs caract√©ristiques principales :

### Quality in Use  

La qualit√© en usage repr√©sente la mani√®re dont le syst√®me r√©pond aux attentes des utilisateurs finaux dans un contexte r√©el. Les crit√®res suivants y sont √©valu√©s :  

#### 1. Effectiveness (Efficacit√©)  
L'application doit permettre aux utilisateurs d‚Äôatteindre leurs objectifs avec pr√©cision. Dans notre cas, l‚Äôalgorithme de Monte Carlo impl√©ment√© doit fournir une estimation fiable de œÄ, avec une marge d'erreur minimale. L‚Äôefficacit√© du programme est renforc√©e par l‚Äôutilisation d‚Äôune approche parall√®le, r√©duisant ainsi le temps de traitement.  

#### 2. Efficiency (Efficience)  
Le programme exploite efficacement les ressources disponibles gr√¢ce √† l'optimisation du parall√©lisme et √† l'architecture Master/Worker. La distribution des calculs sur plusieurs threads et machines am√©liore significativement la rapidit√© d'ex√©cution tout en optimisant l'utilisation du CPU et de la m√©moire.  

#### 3. Satisfaction  
L'exp√©rience utilisateur repose sur plusieurs √©l√©ments :  
- **Usefulness (Utilit√©)** : L‚Äôoutil r√©pond aux besoins sp√©cifiques de calcul distribu√© en proposant une solution √©volutive et adaptable.  
- **Trust (Confiance)** : La pr√©cision des r√©sultats et la gestion des erreurs r√©seau garantissent une certaine fiabilit√©.  
- **Pleasure & Comfort (Plaisir & Confort)** : L‚Äôinterface d‚Äôutilisation et la facilit√© de configuration influencent la satisfaction globale des utilisateurs.  

Dans notre cas, le code Assignements102 est bien moins satisfaisant que Pi en raison de son architecture r√©duisant ses performmances. Quand √† la qualit√© des r√©sultats et de l'interface, les codes sont similaires et ne procure pas de grandes satisfactions en raison d'un r√©sultat approximatif de pi et d'une interface en ligne de commandes.

#### 4. Freedom from Risk (R√©duction des risques)  
- **Economic risk mitigation (Att√©nuation des risques √©conomiques)** : L'optimisation du code permet une meilleure utilisation des ressources mat√©rielles, r√©duisant ainsi les co√ªts d‚Äôexploitation.  
- **Health & safety risk mitigation (Att√©nuation des risques de sant√© et s√©curit√©)** : Le programme √©tant ex√©cut√© dans un environnement contr√¥l√©, ces risques sont limit√©s, sauf en cas de surcharge des ressources mat√©rielles qui pourrait engendrer une instabilit√© du syst√®me.  
- **Environmental risk mitigation (Att√©nuation des risques environnementaux)** : En optimisant l'efficacit√© √©nerg√©tique des calculs, l'application contribue √† une r√©duction de la consommation √©lectrique, particuli√®rement dans un contexte de calcul intensif.  

Nous n'avons aucun cas trait√© par notre code li√© √† cette partie.

#### 5. Context Coverage (Couverture du contexte)  
- **Context completeness (Exhaustivit√© du contexte)** : Le programme doit √™tre capable de s‚Äôadapter √† diff√©rentes architectures (monoposte, cluster, cloud).  
- **Flexibility (Flexibilit√©)** : L‚Äôalgorithme peut √™tre modifi√© et appliqu√© √† d‚Äôautres types de simulations n√©cessitant du calcul distribu√©.  

Les deux codes sont flexibles et peuvent s'effecuter sur plusieurs architectures. Nous avons pu r√©alis√©s avec l'aide des Sockets une de la programmation √† m√©moire paratg√©e et √† m√©moire distribu√©e sur des machines avec des architectures diff√©rentes.

---

### Product Quality  

La qualit√© intrins√®que du produit est √©valu√©e √† travers les crit√®res d√©finis par **ISO 25010**, qui englobent les aspects structurels et fonctionnels du code.  

#### 1. Fonctionnalit√©  
- **Exactitude** : L‚Äôimpl√©mentation de Monte Carlo assure une pr√©cision correcte dans l'estimation de œÄ, valid√©e par des tests de comparaison.  
- **Interop√©rabilit√©** : La solution peut fonctionner sur diff√©rentes infrastructures mat√©rielles et logicielles, ce qui facilite son int√©gration dans divers environnements.  

Nous avons pu observ√© une marge d'erreur minime sur les deux codes nous donnant une valeur approximative de pi √† 10^-4^ pr√®s.

#### 2. Fiabilit√©  
- **Maturit√©** : Le programme a √©t√© test√© sous plusieurs configurations, garantissant une certaine robustesse dans son ex√©cution.  
- **Tol√©rance aux fautes** : La gestion des erreurs est essentielle pour assurer la continuit√© du traitement, notamment en cas de panne d‚Äôun worker. 

La fiabilit√© de Assignement102 est tr√®s limit√©e compar√©e √† Pi qui g√®re beaucoup mieux les threads et √† un code beaucoup plus optimis√©. 

#### 3. Performance et Efficience  
- **Utilisation des ressources** : L‚Äôimpl√©mentation optimise l‚Äôutilisation du processeur gr√¢ce au parall√©lisme.  
- **Scalabilit√©** : Le programme peut √™tre ex√©cut√© sur un grand nombre de threads et de machines sans perte significative de performances. 

Comme dit pour le point pr√©c√©dent, le code de Assignements102 limite √©norm√©ment les capacit√©s de la machines lui donnant une scalabilit√© forte nulle et une scalabitilit√© faible abominable. En revanche le code de Pi g√®re beaucoup mieux les ressources critiques et donc permet √† la machine d'exploiter une tr√®s grande partie de ses ressources pour faire tourner le code.

#### 4. Maintenabilit√©  
- **Modularit√©** : La s√©paration des composants (`Master`, `Worker`, `Pi.java`) am√©liore la lisibilit√© et la r√©utilisabilit√© du code.  
- **Analyse et testabilit√©** : Des tests unitaires et de performance sont n√©cessaires pour garantir la stabilit√© de l‚Äôapplication.  

Malgr√®s un manque de test unitaires. Nous pouvons facilement r√©alis√©s des test d'acceptation et donc v√©rifier que les modifications apport√©es n'ont pas d√©truit le code.

#### 5. S√©curit√©  
- **Protection des donn√©es** : La communication entre Master et Workers via sockets n√©cessite des m√©canismes de s√©curisation pour √©viter les interceptions et falsifications de donn√©es.

Les expr√©riences √©tant √©taient r√©alis√©es sur des r√©seaux 'local' ferm√©s. Il n'y a pas de probl√®mes de confidentialit√©.

---

## R√©f√©rences et sources

- Cours de Programmation Avanc√©e, T. Dufaud.
- Cours de Jos√© Paumard.
- Code assignment102 :
  - Auteur : Karthik Jain (Software Developer, <https://www.krthkj.com>)
  - Source : <https://gist.github.com/krthkj/9c1868c1f69142c2952683ea91ca2a37>
- Code Pi :
  - Auteur : Dr. Steve Kautz, IOWA State University, <https://faculty.sites.iastate.edu/smkautz/>

- Technologies utilis√©es :
  - Java + biblioth√®ques natives - Langage de programmation
  - IntelliJ IDEA - Environnement de d√©veloppement int√©gr√©
  - Visual Studio Code - Environnement de d√©veloppement int√©gr√©
  - Git + GitHub - Gestion de versions et h√©bergement de code
  - StarUML, Draw.io - Mod√©lisation UML
  - librairies python : matplotlib - R√©alisation des graphismes