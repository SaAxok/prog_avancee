# Rapport G√©n√©ral - Partie 2

>[Retour au README](./README.md)

## Table des mati√®res

- [Rapport G√©n√©ral - Partie 2](#rapport-g√©n√©ral---partie-2)
  - [Table des mati√®res](#table-des-mati√®res)
  - [TP4 : Concepts avanc√©s et Monte Carlo](#tp4--concepts-avanc√©s-et-monte-carlo)
  - [Objectif](#objectif)
    - [Master / Worker](#master--worker)
    - [Future](#future)
    - [Acc√©l√©ration (Speedup)](#acc√©l√©ration-speedup)
    - [Scalabilit√©](#scalabilit√©)
    - [Work Stealing Pool](#work-stealing-pool)
    - [Application de l'API Concurrent](#application-de-lapi-concurrent)
    - [M√©thode de Monte Carlo](#m√©thode-de-monte-carlo)
      - [Explication de la Parall√©lisation par T√¢che](#explication-de-la-parall√©lisation-par-t√¢che)
      - [Exemple de Code Parall√©lis√©](#exemple-de-code-parall√©lis√©)
      - [Explication de la Parall√©lisation](#explication-de-la-parall√©lisation)
      - [Avantages de la Parall√©lisation par T√¢che](#avantages-de-la-parall√©lisation-par-t√¢che)
      - [Conclusion](#conclusion)
    - [Analyse des performances de Monte Carlo](#analyse-des-performances-de-monte-carlo)
    - [√âtude de la Scalabilit√©](#√©tude-de-la-scalabilit√©)
      - [Pi.java](#pijava)
      - [Assignments102](#assignments102)
    - [Comparaison Assignment102 vs Pi](#comparaison-assignment102-vs-pi)
      - [Diff√©rences d'erreurs](#diff√©rences-derreurs)
      - [Diff√©rences de paradigmes de programmation](#diff√©rences-de-paradigmes-de-programmation)
      - [Impact sur la Scalabilit√©](#impact-sur-la-scalabilit√©)
  - [Programmation distribu√©e](#programmation-distribu√©e)
    - [Master / Worker en distribu√©](#master--worker-en-distribu√©)
    - [Analyse des Sockets JAVA](#analyse-des-sockets-java)
    - [Monte Carlo Master/Worker Socket](#monte-carlo-masterworker-socket)
      - [Analyse MasterSocket.java](#analyse-mastersocketjava)
      - [Analyse WorkerSocket.java](#analyse-workersocketjava)
  - [R√©f√©rences et sources](#r√©f√©rences-et-sources)

## TP4 : Concepts avanc√©s et Monte Carlo

## Objectif
L'objectif de ce TP est d'explorer diff√©rentes approches pour calculer œÄ en utilisant la m√©thode de Monte Carlo. En comparant les paradigmes de programmation de `Pi.java` et `Assignment102`, nous examinons les performances et l'efficacit√© de la programmation √† m√©moire partag√©e et distribu√©e. Cette √©tude vise √† comprendre comment la parall√©lisation et la distribution des t√¢ches peuvent optimiser les calculs intensifs, tout en mettant en avant les contraintes li√©es √† la synchronisation et √† la gestion des ressources.

### Master / Worker
Le mod√®le **Master/Worker** est une architecture de programmation parall√®le o√π un **master** distribue des t√¢ches √† plusieurs **workers**. Le master coordonne les t√¢ches et collecte les r√©sultats, tandis que les workers ex√©cutent les t√¢ches en parall√®le.

### Future
Un **Future** repr√©sente le r√©sultat d'une op√©ration asynchrone. Il permet de v√©rifier si le calcul est termin√© et de r√©cup√©rer le r√©sultat une fois disponible.

### Acc√©l√©ration (Speedup)
L'**acc√©l√©ration** mesure le gain de performance obtenu en utilisant plusieurs processeurs par rapport √† un seul. Elle est calcul√©e comme le rapport entre le temps d'ex√©cution s√©quentiel et le temps d'ex√©cution parall√®le.

### Scalabilit√©
La **scalabilit√©** est la capacit√© d'un syst√®me √† augmenter ses performances proportionnellement √† l'ajout de ressources (comme des processeurs). Elle peut √™tre **forte** (augmentation lin√©aire) ou **faible** (augmentation sous-lin√©aire).

### Work Stealing Pool
Un **Work Stealing Pool** est une structure de donn√©es o√π les threads volent des t√¢ches √† d'autres threads lorsqu'ils sont inactifs, optimisant ainsi l'utilisation des ressources.

### Application de l'API Concurrent
L'API **Concurrent** de Java fournit des outils pour la programmation parall√®le, comme les **Executors**, les **Futures**, et les **Concurrent Collections**, facilitant la gestion des threads et des t√¢ches.

### M√©thode de Monte Carlo
La m√©thode de **Monte Carlo** est une technique statistique utilis√©e pour estimer des r√©sultats num√©riques en g√©n√©rant des √©chantillons al√©atoires. Elle est souvent utilis√©e pour des simulations complexes.

![illustration pi montecarlo](./assets/pi_montecarlo.png)

La parall√©lisation par t√¢che est une technique utilis√©e pour am√©liorer les performances d'un programme en ex√©cutant plusieurs t√¢ches simultan√©ment sur plusieurs processeurs ou c≈ìurs. Dans le contexte de la m√©thode de Monte Carlo, la parall√©lisation par t√¢che peut √™tre particuli√®rement efficace car cette m√©thode repose sur la g√©n√©ration d'un grand nombre d'√©chantillons al√©atoires ind√©pendants.

#### Explication de la Parall√©lisation par T√¢che

1. **D√©composition en T√¢ches** :
   - **T√¢che** : Une t√¢che est une unit√© de travail ind√©pendante qui peut √™tre ex√©cut√©e s√©par√©ment. Dans le cas de la m√©thode de Monte Carlo, chaque √©chantillon al√©atoire ou groupe d'√©chantillons peut √™tre consid√©r√© comme une t√¢che.
   - **D√©composition** : Le probl√®me global est divis√© en plusieurs t√¢ches plus petites. Par exemple, si vous devez g√©n√©rer 1 000 000 d'√©chantillons, vous pouvez diviser ce travail en 10 t√¢ches de 100 000 √©chantillons chacune.

2. **Ex√©cution Parall√®le** :
   - **Processeurs/C≈ìurs** : Chaque t√¢che est assign√©e √† un processeur ou c≈ìur diff√©rent pour √™tre ex√©cut√©e simultan√©ment. Si vous avez 4 c≈ìurs, vous pouvez ex√©cuter 4 t√¢ches en parall√®le.
   - **Ind√©pendance** : Les t√¢ches doivent √™tre ind√©pendantes les unes des autres pour √©viter les conflits d'acc√®s aux ressources partag√©es. Dans la m√©thode de Monte Carlo, les √©chantillons sont g√©n√©ralement ind√©pendants, ce qui facilite la parall√©lisation.

3. **Synchronisation et Agr√©gation** :
   - **Synchronisation** : Une fois que toutes les t√¢ches sont termin√©es, les r√©sultats doivent √™tre synchronis√©s. Cela peut impliquer la collecte des r√©sultats de chaque t√¢che et leur agr√©gation pour obtenir le r√©sultat final.
   - **Agr√©gation** : Les r√©sultats partiels des diff√©rentes t√¢ches sont combin√©s pour obtenir le r√©sultat global. Par exemple, si chaque t√¢che calcule une estimation partielle, ces estimations peuvent √™tre moyenn√©es pour obtenir l'estimation finale.

#### Exemple de Code Parall√©lis√©

Voici un exemple de parall√©lisation par t√¢che pour le calcul de PI avec la m√©thode de Monte Carlo, utilisant l'API concurrent :

```python
import concurrent.futures
import random

def monte_carlo_task(num_samples):
    inside_circle = 0
    for _ in range(num_samples):
        x, y = random.random(), random.random()
        if x**2 + y**2 <= 1:
            inside_circle += 1
    return inside_circle

def monte_carlo_pi(num_samples, num_tasks):
    samples_per_task = num_samples // num_tasks
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [executor.submit(monte_carlo_task, samples_per_task) for _ in range(num_tasks)]
        inside_circle_counts = [future.result() for future in concurrent.futures.as_completed(futures)]
    total_inside_circle = sum(inside_circle_counts)
    return (total_inside_circle / num_samples) * 4
```

#### Explication de la Parall√©lisation
**D√©composition en T√¢ches :**
- La fonction `monte_carlo_task` repr√©sente une t√¢che individuelle. Elle prend un nombre d'√©chantillons (`num_samples`) et compte combien de ces √©chantillons tombent √† l'int√©rieur d'un cercle unit√©.
- La fonction `monte_carlo_pi` divise le nombre total d'√©chantillons (`num_samples`) en plusieurs t√¢ches (`num_tasks`). Chaque t√¢che traite un sous-ensemble des √©chantillons.

**Ex√©cution Parall√®le :**
- **ProcessPoolExecutor** : La biblioth√®que `concurrent.futures` est utilis√©e pour cr√©er un pool de processus (`ProcessPoolExecutor`). Cela permet d'ex√©cuter plusieurs t√¢ches en parall√®le sur plusieurs processeurs ou c≈ìurs.
- **submit** : La m√©thode `executor.submit` soumet chaque t√¢che (`monte_carlo_task`) au pool de processus. Chaque t√¢che est ex√©cut√©e de mani√®re ind√©pendante et en parall√®le.
- **futures** : Les objets `future` repr√©sentent les t√¢ches en cours d'ex√©cution. Ils permettent de r√©cup√©rer les r√©sultats une fois que les t√¢ches sont termin√©es.

**Synchronisation et Agr√©gation :**
- **as_completed** : La m√©thode `concurrent.futures.as_completed` permet de r√©cup√©rer les r√©sultats des t√¢ches au fur et √† mesure qu'elles se terminent.
- **result** : La m√©thode `future.result()` r√©cup√®re le r√©sultat de chaque t√¢che. Les r√©sultats sont ensuite agr√©g√©s pour obtenir le nombre total de points √† l'int√©rieur du cercle.
- **Calcul Final** : Le r√©sultat final est calcul√© en utilisant la formule de Monte Carlo pour estimer œÄ.

#### Avantages de la Parall√©lisation par T√¢che

- **Performance** : R√©duit le temps d'ex√©cution en utilisant plusieurs processeurs ou c≈ìurs.
- **Scalabilit√©** : Peut √™tre facilement adapt√© pour utiliser plus de ressources mat√©rielles.
- **Simplicit√©** : Les t√¢ches ind√©pendantes sont plus faciles √† g√©rer et √† synchroniser.

#### Conclusion

La parall√©lisation par t√¢che est une technique puissante pour am√©liorer les performances des simulations de Monte Carlo en tirant parti des architectures multi-c≈ìurs modernes. En d√©composant le probl√®me en t√¢ches ind√©pendantes et en les ex√©cutant en parall√®le, on peut obtenir des r√©sultats plus rapidement et de mani√®re plus efficace.

### Analyse des performances de Monte Carlo
La m√©thode de Monte Carlo peut b√©n√©ficier d'une bonne scalabilit√© gr√¢ce √† son parall√©lisme naturel. Cependant, les limitations num√©riques et mat√©rielles peuvent rapidement limiter les gains de performance.

### √âtude de la Scalabilit√©

Pour √©tudier la scalabilit√©, nous avons choisi un nombre d'essais suffisamment grand pour mesurer le temps d'ex√©cution de mani√®re significative. Nous avons test√© avec plusieurs workers et compar√© les r√©sultats. Dans ce cas il s'agissait de 12 millions d'op√©rations.

#### Pi.java

Un script a √©t√© mis en place pour r√©aliser un graphique de la scalabilit√© forte de Pi.java, en modifiant le nombre de workers. Nous avons observ√© qu'au-del√† de 8 c≈ìurs, le speedup diminue en raison des limitations mat√©rielles de la machine.

**Scalabilit√© forte sur une machine en G26**
![Scalabilit√© forte Pi G26](./TP4/results_scalabilite/scal_forte_G26_pi.png)

**Scalabilit√© forte sur mon MacBook M1 pro**
![Scalabilit√© forte Pi Mac](./TP4/results_scalabilite/scal_forte_mac_pi.png)

**Scalabilit√© faible sur une machine en G26**
![Scalabilit√© faible Pi G26](./TP4/results_scalabilite/scal_faible_G26_pi.png)

**Scalabilit√© faible sur mon MacBook M1 pro**
![Scalabilit√© faible Pi Mac](./TP4/results_scalabilite/scal_faible_mac_pi.png)

- **Parall√©lisation bas√©e sur un pool de threads fixe :**
  Le programme utilise `Executors.newFixedThreadPool(numWorkers)` pour cr√©er un pool de `numWorkers` threads, ce qui permet un contr√¥le efficace du parall√©lisme en limitant le nombre de threads actifs.

- **R√©partition des t√¢ches :**
  Chaque worker (`Callable<Long>`) ex√©cute une fraction du nombre total d‚Äôit√©rations et renvoie un r√©sultat partiel. L‚Äôagr√©gation des r√©sultats se fait via `Future.get()`, qui agit comme une **barri√®re implicite** (les r√©sultats sont collect√©s une fois que tous les threads ont termin√©).

- **Avantages de cette approche :**
  - Les t√¢ches sont bien √©quilibr√©es entre les threads (`totalCount / numWorkers` par worker).
  - L'utilisation de `invokeAll()` garantit que tous les threads travaillent en parall√®le avant de r√©cup√©rer les r√©sultats.
  - Contrairement √† `Assignment102.java`, le programme **r√©duit le nombre de t√¢ches** en regroupant plusieurs it√©rations par worker, √©vitant ainsi la surcharge li√©e √† la gestion d‚Äôun trop grand nombre de petites t√¢ches.

- **Limites observ√©es :**
  - Au-del√† de 8 threads, le **speedup diminue** en raison de la saturation des ressources CPU et des co√ªts de synchronisation.
  - L‚Äôappel `Future.get()` bloque le thread principal jusqu'√† ce que tous les workers aient termin√©, ce qui peut cr√©er un goulot d‚Äô√©tranglement si certains threads prennent plus de temps que d‚Äôautres.
  - L'utilisation de `Random` par chaque thread peut provoquer une contention si plusieurs threads g√©n√®rent des nombres al√©atoires en parall√®le.

#### Assignments102

Le script pr√©c√©dent a √©t√© r√©utilis√© pour `Assignments102.java`. Ce paradigme fonctionne diff√©remment et les mesures r√©v√®lent qu'il est moins efficace. Le speedup de la scalabilit√© forte du code `Assignments102` est proche de la scalabilit√© faible de `Pi.java`.

**Scalabilit√© forte sur une machine en G26**
![Scalabilit√© forte Assignments102 G26](./TP4/results_scalabilite/)

**Scalabilit√© forte sur mon MacBook M1 pro**
![Scalabilit√© forte Assignments102 Mac](./TP4/results_scalabilite/scal_forte_mac_assignments102.png)

**Scalabilit√© faible sur une machine en G26**
![Scalabilit√© faible Assignments102 G26](./TP4/results_scalabilite/)

**Scalabilit√© faible sur mon MacBook M1 pro**
![Scalabilit√© faible Assignments102 Mac](./TP4/results_scalabilite/)

- **Trop de t√¢ches cr√©√©es (1 t√¢che par point simul√©) :**
  Chaque simulation de point (x, y) est soumise individuellement au pool de threads, ce qui entra√Æne une surcharge importante sur le gestionnaire de t√¢ches. La cr√©ation et la gestion d'un grand nombre de threads tr√®s courts g√©n√®rent un overhead significatif et r√©duisent l'efficacit√© globale du programme.

- **Probl√®me de synchronisation sur `AtomicInteger` :**
  L'utilisation de `AtomicInteger.incrementAndGet()` pour compter les points dans le cercle cr√©e une contention m√©moire entre threads. Chaque mise √† jour √©tant atomique, cela ralentit l'acc√®s concurrentiel. Une alternative plus performante serait l'utilisation de `LongAdder`, qui r√©duit les conflits en r√©partissant les mises √† jour sur plusieurs variables internes.

Globalement, `Pi.java` est mieux parall√©lis√© que `Assignment102.java`, car il minimise la surcharge de cr√©ation de threads et utilise un **mod√®le de travail bien √©quilibr√©** pour exploiter efficacement les ressources CPU.

### Comparaison Assignment102 vs Pi

#### Diff√©rences d'erreurs
L'erreur dans l'estimation de œÄ varie en fonction de la m√©thode utilis√©e :
- **`Assignment102`** : Chaque point g√©n√©r√© est trait√© individuellement, ce qui entra√Æne une grande variabilit√© due aux fluctuations al√©atoires et aux conflits d'acc√®s sur une variable partag√©e. La convergence est plus lente et l'erreur plus importante.
- **`Pi.java`** : La g√©n√©ration de points est r√©partie en lots et ex√©cut√©e en parall√®le, r√©duisant l'impact des erreurs al√©atoires et am√©liorant la stabilit√© de l'estimation de œÄ.

Une analyse des r√©sultats montre que `Pi.java` produit des valeurs plus pr√©cises avec une variance plus faible compar√©e √† `Assignment102`.

#### Diff√©rences de paradigmes de programmation

1. **Approche `Assignment102` : Parall√©lisme de fine granularit√©**
   - Chaque point al√©atoire est trait√© comme une t√¢che distincte.
   - Utilisation intensive de `AtomicInteger` pour la synchronisation, entra√Ænant des ralentissements dus √† la contention m√©moire.
   - Nombre tr√®s √©lev√© de t√¢ches l√©g√®res soumises √† un gestionnaire de threads, augmentant la surcharge du syst√®me.

2. **Approche `Pi.java` : Parall√©lisme par regroupement de t√¢ches**
   - Chaque thread traite un bloc d'√©chantillons, r√©duisant le nombre de t√¢ches cr√©√©es.
   - Synchronisation minimale gr√¢ce √† l'agr√©gation locale avant soumission des r√©sultats.
   - Utilisation optimis√©e des c≈ìurs processeur avec un pool de threads √©quilibr√©.

#### Impact sur la Scalabilit√©
- **`Assignment102`** pr√©sente une scalabilit√© limit√©e en raison du nombre excessif de t√¢ches et de la contention sur `AtomicInteger`.
- **`Pi.java`** est plus efficace car il minimise les conflits et optimise l'utilisation des ressources CPU.

Lors de la comparaison entre `Pi.java` et `Assignment102`, il est clair que `Pi.java` est plus efficace. `Pi.java` g√®re mieux les t√¢ches et √©vite les conflits entre les diff√©rentes parties du programme. Pour des projets o√π il faut bien utiliser les ressources √† disposition et obtenir des r√©sultats pr√©cis. Cette m√©thode simplifie la gestion des t√¢ches et assure une bonne coordination, ce qui est tr√®s utile quand les ressources sont limit√©es.  
En conclusion, la m√©thode `Pi.java` surpasse `Assignment102` en termes d'efficacit√© et de pr√©cision gr√¢ce √† une meilleure gestion des t√¢ches et une r√©duction des conflits d'acc√®s concurrentiels. 

## Programmation distribu√©e

### Master / Worker en distribu√©
Dans un environnement distribu√©, le **master** envoie des messages aux **workers** via un r√©seau. Les workers ex√©cutent les t√¢ches en parall√®le et renvoient les r√©sultats au master.

### Analyse des Sockets JAVA

![Image UML WorkerSocket](./assets/MW_Socket_UML.png)

### Monte Carlo Master/Worker Socket

Pour calculer œÄ et √©tablir les communications Master/Worker, les changements suivants ont √©t√© apport√©s :
- Le **Master** distribue le nombre total de points √† g√©n√©rer aux **Workers**.
- Chaque **Worker** g√©n√®re des points al√©atoires et compte ceux qui tombent dans un quart de cercle.
- Les r√©sultats sont renvoy√©s au **Master**, qui les agr√®ge pour estimer œÄ.

#### Analyse MasterSocket.java

**√âtapes principales dans le code**
**Initialisation des workers** :
- Le master demande combien de workers (processus) seront utilis√©s. Il ouvre un socket (canal de communication) pour chaque worker sur un port donn√©.
**Envoi des t√¢ches aux workers** :
- Chaque worker re√ßoit le nombre total de points √† g√©n√©rer pour l'estimation de ùúã.

**Traitement par les workers** :
- Les workers g√©n√®rent des points al√©atoires dans un carr√©, comptent ceux qui tombent dans un quart de cercle et renvoient leurs r√©sultats au master.

**R√©cup√©ration des r√©sultats** :
- Le master collecte les r√©sultats des workers via leurs sockets respectifs.
- Il combine ces r√©sultats pour calculer la valeur approximative de ùúã.

**Affichage des r√©sultats** :
- Le master affiche ùúã, l'erreur relative, et les statistiques de performance (dur√©e, nombre de points, etc.).
- L'utilisateur peut choisir de r√©p√©ter la simulation.

**Fermeture des sockets** :
- Une fois la simulation termin√©e, les sockets entre le master et les workers sont ferm√©s proprement.

**Sockets :**
- **Socket c√¥t√© master** : Utilis√© pour envoyer des t√¢ches et recevoir des r√©sultats.
- **Socket c√¥t√© worker (non montr√© ici)** : √âcoute les messages du master, ex√©cute la t√¢che, puis renvoie le r√©sultat.

**Flux g√©n√©ral de la communication**
**Master :**
- Connecte un socket pour chaque worker.
- Envoie une t√¢che √† chaque worker via un flux d'√©criture (PrintWriter).
- Lit les r√©sultats des workers via un flux de lecture (BufferedReader).

**Workers (c√¥t√© serveur, non montr√© ici) :**
- √âcoute sur un port sp√©cifique.
- Re√ßoit les donn√©es envoy√©es par le master.
- Effectue le calcul et renvoie le r√©sultat.

#### Analyse WorkerSocket.java

**√âtapes principales du code Worker**
**Configuration du Worker :**
- Le Worker d√©marre un serveur socket sur un port donn√© (par d√©faut 25545 ou sp√©cifi√© en argument).
- Il attend une connexion entrante du Master via le socket.

**Communication avec le Master :**
- Une fois connect√©, le Worker √©coute les messages du Master en utilisant un flux d'entr√©e (BufferedReader).
- Apr√®s avoir re√ßu les donn√©es, il effectue le calcul n√©cessaire.

**Envoi des r√©sultats au Master :**
- Le Worker utilise un flux de sortie (PrintWriter) pour renvoyer les r√©sultats du calcul au Master.

**Gestion des messages et fin de t√¢che :**
- Si le message re√ßu est "END", le Worker arr√™te son ex√©cution.
- Sinon, il continue √† traiter les donn√©es envoy√©es par le Master.

**Flux de communication entre Master et Worker**
**C√¥t√© Master :**
- **Client socket :** Le Master initie la connexion et envoie des t√¢ches aux Workers. Le Master attend les r√©ponses des Workers pour agr√©ger les r√©sultats.

**C√¥t√© Worker :**
- **Serveur socket :** Le Worker accepte les connexions du Master. Chaque message re√ßu est interpr√©t√© comme une instruction (par exemple, combien de points g√©n√©rer pour la m√©thode Monte Carlo). Une fois le calcul termin√©, le r√©sultat est renvoy√© au Master.

---

## R√©f√©rences et sources

- Cours de Programmation Avanc√©e, T. Dufaud.
- Cours de Jos√© Paumard.
- Code assignment102 :
  - Auteur : Karthik Jain (Software Developer, <https://www.krthkj.com>)
  - Source : <https://gist.github.com/krthkj/9c1868c1f69142c2952683ea91ca2a37>
- Code Pi :
  - Auteur : Dr. Steve Kautz, IOWA State University, <https://faculty.sites.iastate.edu/smkautz/>

- Technologies utilis√©es :
  - Java + biblioth√®ques natives - Langage de programmation
  - IntelliJ IDEA - Environnement de d√©veloppement int√©gr√©
  - Visual Studio Code - Environnement de d√©veloppement int√©gr√©
  - Git + GitHub - Gestion de versions et h√©bergement de code
  - StarUML, Draw.io - Mod√©lisation UML
  - librairies python : matplotlib - R√©alisation des graphismes